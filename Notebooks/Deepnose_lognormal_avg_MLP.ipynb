{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Add the ./src folder to the Python module search path\n",
    "sys.path.append(os.path.join(current_dir, '..', 'src'))\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an MLP and a SVR model for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 203)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../Data'\n",
    "\n",
    "feature_file = 'deepnose_features.npy'\n",
    "CID_file = 'molecules_train_cid.npy'\n",
    "\n",
    "mixture_file = 'Mixure_Definitions_Training_set.csv'\n",
    "training_task_file = 'TrainingData_mixturedist.csv'\n",
    "\n",
    "# Deepnose features\n",
    "features = np.load(os.path.join(input_path, feature_file))\n",
    "# Training dataframe\n",
    "training_set = pd.read_csv(os.path.join(input_path, training_task_file))\n",
    "\n",
    "# Mapping helper files\n",
    "mixtures_IDs = pd.read_csv(os.path.join(input_path, mixture_file))\n",
    "features_CIDs = np.load(os.path.join(input_path, CID_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-8\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "features = scaler.fit_transform(np.log(features + epsilon))\n",
    "\n",
    "# Map CID to 96 dim features:\n",
    "CID2features =  {CID: features[i] for i, CID in enumerate(features_CIDs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, num_mixtures, all_pairs_CIDs = format_Xy(training_set,  mixtures_IDs, CID2features, method = 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input pairs to a suitable format for training\n",
    "X_pairs = np.array([(np.concatenate((x1, x2))) for x1, x2 in X])\n",
    "y_true = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [get_euclidean_distance(m[0], m[1]) for m in X]\n",
    "similarities = [get_cosine_similarity(m[0], m[1]) for m in X]\n",
    "angles = [get_cosine_angle(m[0], m[1]) for m in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_monos = [ len( set(pair[0]).intersection(set(pair[1]))) for pair in all_pairs_CIDs]\n",
    "diff_monos = [ len( set(pair[0]).difference(set(pair[1]))) for pair in all_pairs_CIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = training_set['Dataset'].to_numpy()\n",
    "encoder = OneHotEncoder()\n",
    "data_arr = encoder.fit_transform(datasets.reshape(-1, 1))\n",
    "data_arr = data_arr.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features above\n",
    "X_features = np.hstack((X_pairs, np.array(distances).reshape(500, 1), \n",
    "                        np.array(similarities).reshape(500, 1), \n",
    "                        np.array(angles).reshape(500, 1), \n",
    "                        np.array(shared_monos).reshape(500, 1), \n",
    "                        np.array(diff_monos).reshape(500, 1), \n",
    "                        np.array(num_mixtures), \n",
    "                        data_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "seed = 314159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPRegressor(hidden_layer_sizes=(100, 50),  # Hidden layers, as input is ~200, we use 100 and 50\n",
    "                         activation='relu', \n",
    "                         solver='adam',  \n",
    "                         alpha=0.3,  # Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss.\n",
    "                         max_iter=1000,  # Set the maximum number of iterations for weight optimization\n",
    "                         random_state=seed)  \n",
    "\n",
    "# Custom scoring function for RMSE\n",
    "rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False, squared=False)\n",
    "\n",
    "# 10-fold cross-validation with RMSE as the scoring metric\n",
    "cv_scores = cross_val_score(mlp_model, X_features, y_true, cv=n_folds, scoring=rmse_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation RMSE: -0.16496821102103204\n",
      "\n",
      "Std Cross-validation RMSE: 0.024555685887717907\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean cross-validation RMSE:\", cv_scores.mean())\n",
    "print()\n",
    "print(\"Std Cross-validation RMSE:\", cv_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between y_true and y_pred: 0.8241040354845336\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on the entire dataset\n",
    "mlp_model.fit(X_features, y_true)\n",
    "\n",
    "# Make predictions on the dataset\n",
    "y_pred = mlp_model.predict(X_features)\n",
    "\n",
    "# Calculate the correlation between y_true and y_pred\n",
    "correlation = r2_score(y_true, y_pred)\n",
    "print(\"Correlation between y_true and y_pred:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over-fitting to the data; to see that, we do a train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between y_true and y_pred for the test set: -0.1757289855573687\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_true, test_size=0.1, random_state=seed)\n",
    "\n",
    "# Create an MLP Regressor with L1 regularization\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(100, 50), \n",
    "                         activation='relu', \n",
    "                         solver='adam', \n",
    "                         alpha=0.3,\n",
    "                         max_iter=1000, \n",
    "                         random_state=seed)\n",
    "                        \n",
    "\n",
    "# Fit the model on the training set (450 samples)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set (50 samples)\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "\n",
    "# Calculate the correlation between y_true and y_pred for the test set\n",
    "correlation = r2_score(y_test, y_pred)\n",
    "print(\"Correlation between y_true and y_pred for the test set:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR\n",
    "\n",
    "We used the Radial Basis Function (RBF) kernel as it can handle non-linear relationships in the data effectively, and regress based on similarity of data in all dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SVR model with the RBF kernel\n",
    "svr = SVR(kernel='rbf')\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV \n",
    "param_grid = {'C': [0.1, 1, 10, 100], # regularization parameter\n",
    "              'gamma': [0.01, 0.1, 1, 'auto'], # Kernel Coefficient, 1/n_features for 'auto'\n",
    "              'epsilon': [0.01, 0.1, 0.2]} # width of the insensitive region around the true values, the tube\n",
    "\n",
    "# Perform GridSearchCV or RandomizedSearchCV to tune the hyperparameters\n",
    "grid_search = GridSearchCV(svr, param_grid, cv=10, scoring='r2')\n",
    "grid_search.fit(X_features, y_true)\n",
    "\n",
    "# Get the best hyperparameters and the best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.13136123218098572\n",
      "Correlation: 0.5192976367021426\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_true, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Fit the best estimator on the training data\n",
    "best_estimator.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", np.sqrt(mse))\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Correlation:\", np.sqrt(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
