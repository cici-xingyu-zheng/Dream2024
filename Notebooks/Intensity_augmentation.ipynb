{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Add the ./src folder to the Python module search path\n",
    "sys.path.append(os.path.join(current_dir, '..', 'src'))\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../Data'\n",
    "\n",
    "feature_file = 'deepnose_features.npy'\n",
    "CID_file = 'molecules_train_cid.npy'\n",
    "\n",
    "mixture_file = 'Mixure_Definitions_Training_set.csv'\n",
    "intensity_file = 'Mixure_Definitions_Intensity_Training_set.csv'\n",
    "training_task_file = 'TrainingData_mixturedist.csv'\n",
    "\n",
    "features = np.load(os.path.join(input_path, feature_file))\n",
    "training_set = pd.read_csv(os.path.join(input_path, training_task_file))\n",
    "mixtures_IDs = pd.read_csv(os.path.join(input_path, mixture_file))\n",
    "molecule_intensities = pd.read_csv(os.path.join(input_path, intensity_file))\n",
    "\n",
    "features_CIDs = np.load(os.path.join(input_path, CID_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_features = np.load(os.path.join(input_path, 'Extra/deepnose_features_extRavia.npy'))\n",
    "extended_training_set = pd.read_csv(os.path.join(input_path, 'Extra/extended_training_set.csv'))\n",
    "extended_mixture_IDs = pd.read_csv(os.path.join(input_path, 'Extra/extended_mixture_IDs.csv'))\n",
    "extended_molecule_intensities = pd.read_csv(os.path.join(input_path, 'Extra/extended_molecule_intensites.csv'))\n",
    "extended_features_CIDs = np.load(os.path.join(input_path, 'Extra/extended_ravia_cid.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "epsilon = 1e-8\n",
    "features = scaler.fit_transform(np.log(features + epsilon))\n",
    "CID2features =  {CID: features[i] for i, CID in enumerate(features_CIDs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_features = scaler.fit_transform(np.log(extended_features + epsilon))\n",
    "extended_CID2features =  {CID: extended_features[i] for i, CID in enumerate(extended_features_CIDs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlapped_CIDs = list(set(extended_features_CIDs) & set(features_CIDs))\n",
    "# overlapped_CIDs[2]\n",
    "# extended_CID2features[7685]\n",
    "# CID2features[7685]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in extended_CID2features.items():\n",
    "        if key not in CID2features:\n",
    "            CID2features[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare for `X_features` and `y_true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaling constant\n",
    "scaling_constant = 2 # Adjust this value as needed\n",
    "\n",
    "# Get the list of column names containing \"CID\"\n",
    "cid_columns = [col for col in molecule_intensities.columns if 'CID' in col]\n",
    "\n",
    "# Create a mask to identify rows where \"Dataset\" is in ['Snitz 1', 'Snitz 2', 'Bushdid']\n",
    "mask = molecule_intensities['Dataset'].isin(['Snitz 1', 'Snitz 2', 'Bushdid'])\n",
    "\n",
    "# Scale the values of \"CID\" columns for the selected rows\n",
    "molecule_intensities.loc[mask, cid_columns] *= scaling_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "num_monos = []\n",
    "CIDs_all = []\n",
    "\n",
    "for _, row in training_set.iterrows():\n",
    "    mixture1 = combine_molecules_intensity_weighed(label=row['Mixture 1'], dataset=row['Dataset'],\n",
    "                                            mixtures_IDs=mixtures_IDs, CID2features=CID2features,\n",
    "                                            mixtures_intensities= molecule_intensities)\n",
    "    mixture2 = combine_molecules_intensity_weighed(label=row['Mixture 2'], dataset=row['Dataset'],\n",
    "                                            mixtures_IDs=mixtures_IDs, CID2features=CID2features,\n",
    "                                            mixtures_intensities= molecule_intensities)\n",
    "    X.append((mixture1, mixture2))\n",
    "    y.append(row['Experimental Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, num_mixtures, all_pairs_CIDs = format_Xy(training_set,  mixtures_IDs, CID2features, method = 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each `x` in `X` contains a two vector tuple `(mixture_1, mixture_2)`, index ordered same way as `training_set`\n",
    "- `method` specifies the ways to create the mixture embeeding from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input pairs to a suitable format for training\n",
    "X_pairs = np.array([(np.concatenate((x1, x2))) for x1, x2 in X])\n",
    "y_true = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [get_euclidean_distance(m[0], m[1]) for m in X]\n",
    "similarities = [get_cosine_similarity(m[0], m[1]) for m in X]\n",
    "angles = [get_cosine_angle(m[0], m[1]) for m in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_monos = [ len( set(pair[0]).intersection(set(pair[1]))) for pair in all_pairs_CIDs]\n",
    "diff_monos = [ len( set(pair[0]).difference(set(pair[1]))) for pair in all_pairs_CIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = training_set['Dataset'].to_numpy()\n",
    "encoder = OneHotEncoder()\n",
    "data_arr = encoder.fit_transform(datasets.reshape(-1, 1))\n",
    "data_arr = data_arr.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add all information above\n",
    "X_features = np.hstack((X_pairs, np.array(distances).reshape(500, 1), \n",
    "                        np.array(similarities).reshape(500, 1), \n",
    "                        np.array(angles).reshape(500, 1), \n",
    "                        np.array(shared_monos).reshape(500, 1), \n",
    "                        np.array(diff_monos).reshape(500, 1), \n",
    "                        np.array(num_mixtures), \n",
    "                        data_arr,\n",
    "                        np.zeros((500, 2)) # this is given the addition of the new dataset// changed to 2 after filtiering out exp1\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare `X_features_aug` and `y_true_aug`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Exp1', 'Exp2', 'Exp6'], dtype=object)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_training_set['Dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Mixture 1</th>\n",
       "      <th>Mixture 2</th>\n",
       "      <th>Experimental Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exp1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>38.477273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exp1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>44.840909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exp1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>31.022727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exp1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Exp1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>65.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>Exp6</td>\n",
       "      <td>294</td>\n",
       "      <td>294</td>\n",
       "      <td>78.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Exp6</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>68.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Exp6</td>\n",
       "      <td>296</td>\n",
       "      <td>296</td>\n",
       "      <td>76.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>Exp6</td>\n",
       "      <td>297</td>\n",
       "      <td>297</td>\n",
       "      <td>70.588234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>Exp6</td>\n",
       "      <td>298</td>\n",
       "      <td>298</td>\n",
       "      <td>76.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>315 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset  Mixture 1  Mixture 2  Experimental Values\n",
       "0      Exp1          1          3            38.477273\n",
       "1      Exp1          1          8            44.840909\n",
       "2      Exp1          1         11            31.022727\n",
       "3      Exp1          2          5            41.000000\n",
       "4      Exp1          2          7            65.181818\n",
       "..      ...        ...        ...                  ...\n",
       "310    Exp6        294        294            78.000000\n",
       "311    Exp6        295        295            68.000000\n",
       "312    Exp6        296        296            76.000000\n",
       "313    Exp6        297        297            70.588234\n",
       "314    Exp6        298        298            76.000000\n",
       "\n",
       "[315 rows x 4 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_training_set = extended_training_set[extended_training_set['Dataset']!= 'Exp1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total extended data to use: 220\n"
     ]
    }
   ],
   "source": [
    "print(f'total extended data to use: {len(extended_training_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_extended = []\n",
    "y_extended = []\n",
    "num_monos = []\n",
    "CIDs_all = []\n",
    "\n",
    "for _, row in extended_training_set.iterrows():\n",
    "    mixture1 = combine_molecules_intensity_weighed(label=row['Mixture 1'], dataset=row['Dataset'],\n",
    "                                            mixtures_IDs=extended_mixture_IDs, CID2features=CID2features,\n",
    "                                            mixtures_intensities= extended_molecule_intensities)\n",
    "    mixture2 = combine_molecules_intensity_weighed(label=row['Mixture 2'], dataset=row['Dataset'],\n",
    "                                            mixtures_IDs=extended_mixture_IDs, CID2features=CID2features,\n",
    "                                            mixtures_intensities= extended_molecule_intensities)\n",
    "    X_extended.append((mixture1, mixture2))\n",
    "    y_extended.append(row['Experimental Values']/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will have a total of 220 extra Ravia samples.\n"
     ]
    }
   ],
   "source": [
    "print(f'We will have a total of {len(y_extended)} extra Ravia samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_extended = np.array([(np.concatenate((x1, x2))) for (x1, x2) in X_extended])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, extra_num_mixtures, extend_pairs_CIDs = format_Xy(extended_training_set,  extended_mixture_IDs, CID2features, method = 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each `x` in `X` contains a two vector tuple `(mixture_1, mixture_2)`, index ordered same way as `training_set`\n",
    "- `method` specifies the ways to create the mixture embeeding from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_extended = np.array(y_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_e = [get_euclidean_distance(m[0], m[1]) for m in X_extended]\n",
    "similarities_e = [get_cosine_similarity(m[0], m[1]) for m in X_extended]\n",
    "angles_e = [get_cosine_angle(m[0], m[1]) for m in X_extended]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_monos_e = [ len( set(pair[0]).intersection(set(pair[1]))) for pair in extend_pairs_CIDs]\n",
    "diff_monos_e = [ len( set(pair[0]).difference(set(pair[1]))) for pair in extend_pairs_CIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_e = extended_training_set['Dataset'].to_numpy()\n",
    "encoder = OneHotEncoder()\n",
    "data_arr_e = encoder.fit_transform(datasets_e.reshape(-1, 1))\n",
    "data_arr_e = data_arr_e.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add all information above\n",
    "X_features_extended = np.hstack((X_extended, np.array(distances_e).reshape(220, 1), \n",
    "                        np.array(similarities_e).reshape(220, 1), \n",
    "                        np.array(angles_e).reshape(220, 1), \n",
    "                        np.array(shared_monos_e).reshape(220, 1), \n",
    "                        np.array(diff_monos_e).reshape(220, 1), \n",
    "                        np.array(extra_num_mixtures), \n",
    "                        np.zeros((220, 4)), # this is given the addition of the new dataset\n",
    "                        data_arr_e\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_num = 50\n",
    "indices = np.random.choice(len(y_true_extended), size=aug_num, replace=False)\n",
    "X_features_aug = X_features_extended[indices]\n",
    "y_true_aug = np.array(y_true_extended)[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 206 and the array at index 1 has size 205",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m test_indices_list \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Keep track of the test indices in each fold\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Stack the original X and augmented X_pool\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m stacked_X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_features_aug\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Stack the original y_true and augmented y_pool\u001b[39;00m\n\u001b[1;32m     12\u001b[0m stacked_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((y_true, y_true_aug))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback/lib/python3.11/site-packages/numpy/core/shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 206 and the array at index 1 has size 205"
     ]
    }
   ],
   "source": [
    "seed = 314159\n",
    "n_folds = 10\n",
    "\n",
    "rf_pred_list = []\n",
    "xgb_pred_list = []\n",
    "y_true_list = []\n",
    "test_indices_list = []  # Keep track of the test indices in each fold\n",
    "\n",
    "# Stack the original X and augmented X_pool\n",
    "stacked_X = np.vstack((X_features, X_features_aug))\n",
    "# Stack the original y_true and augmented y_pool\n",
    "stacked_y = np.concatenate((y_true, y_true_aug))\n",
    "\n",
    "# Get the number of original samples\n",
    "n_original_samples = X_features.shape[0]\n",
    "\n",
    "# Perform k-fold cross-validation:\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "for train_index, test_index in kf.split(stacked_X):\n",
    "    X_train, X_test = stacked_X[train_index], stacked_X[test_index]\n",
    "    y_train, y_test = stacked_y[train_index], stacked_y[test_index]\n",
    "\n",
    "    # Get the original test indices\n",
    "    original_test_index = test_index[test_index < n_original_samples]\n",
    "    \n",
    "    # Train the Random Forest regressor\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=seed)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Train the XGBoost regressor\n",
    "    xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=seed)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the original X_features\n",
    "    rf_pred = rf.predict(stacked_X[original_test_index])\n",
    "    xgb_pred = xgb_model.predict(stacked_X[original_test_index])\n",
    "    \n",
    "    rf_pred_list.extend(rf_pred)\n",
    "    xgb_pred_list.extend(xgb_pred)\n",
    "    y_true_list.extend(y_true[original_test_index])  # Use the original y_true for evaluation\n",
    "    test_indices_list.extend(original_test_index)  # Store the original test indices\n",
    "\n",
    "# Store the predictions and actual values\n",
    "results_df = pd.DataFrame({\n",
    "    'test_index': test_indices_list,\n",
    "    'rf_pred': rf_pred_list,\n",
    "    'xgb_pred': xgb_pred_list,\n",
    "    'y_true': y_true_list\n",
    "})\n",
    "\n",
    "# Merge the results with the training_set df\n",
    "training_set = training_set.merge(results_df, left_index=True, right_on='test_index')\n",
    "training_set.drop('test_index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - R: 0.610\n",
      "Random Forest - RMSE: 0.124\n",
      "\n",
      "XGBoost - R: 0.543\n",
      "XGBoost - RMSE: 0.133\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation and R^2 for Random Forest\n",
    "rf_corr = np.corrcoef(rf_pred_list, y_true_list)[0, 1]\n",
    "rf_rmse = np.sqrt(mean_squared_error(np.array(y_true_list), np.array(rf_pred_list)))\n",
    "\n",
    "print(f\"Random Forest - R: {rf_corr:.3f}\")\n",
    "print(f\"Random Forest - RMSE: {rf_rmse:.3f}\")\n",
    "print()\n",
    "# Calculate the correlation and R^2 for XGBoost\n",
    "xgb_corr = np.corrcoef(xgb_pred_list, y_true_list)[0, 1]\n",
    "xgb_rmse = np.sqrt(mean_squared_error(np.array(y_true_list), np.array(xgb_pred_list)))\n",
    "\n",
    "print(f\"XGBoost - R: {xgb_corr:.3f}\")\n",
    "print(f\"XGBoost - RMSE: {xgb_rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, num_mixtures, all_pairs_CIDs = format_Xy(training_set,  mixtures_IDs, CID2features, method = 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each `x` in `X` contains a two vector tuple `(mixture_1, mixture_2)`, index ordered same way as `training_set`\n",
    "- `method` specifies the ways to create the mixture embeeding from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input pairs to a suitable format for training\n",
    "X_pairs = np.array([(np.concatenate((x1, x2))) for x1, x2 in X])\n",
    "y_true = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [get_euclidean_distance(m[0], m[1]) for m in X]\n",
    "similarities = [get_cosine_similarity(m[0], m[1]) for m in X]\n",
    "angles = [get_cosine_angle(m[0], m[1]) for m in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_monos = [ len( set(pair[0]).intersection(set(pair[1]))) for pair in all_pairs_CIDs]\n",
    "diff_monos = [ len( set(pair[0]).difference(set(pair[1]))) for pair in all_pairs_CIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this we will need to augment:\n",
    "datasets = training_set['Dataset'].to_numpy()\n",
    "encoder = OneHotEncoder()\n",
    "data_arr = encoder.fit_transform(datasets.reshape(-1, 1))\n",
    "data_arr = data_arr.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add all information above\n",
    "X_features = np.hstack((X_pairs, np.array(distances).reshape(500, 1), \n",
    "                        np.array(similarities).reshape(500, 1), \n",
    "                        np.array(angles).reshape(500, 1), \n",
    "                        np.array(shared_monos).reshape(500, 1), \n",
    "                        np.array(diff_monos).reshape(500, 1), \n",
    "                        np.array(num_mixtures), \n",
    "                        data_arr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
