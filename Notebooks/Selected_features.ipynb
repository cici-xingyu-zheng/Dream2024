{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Add the ./src folder to the Python module search path\n",
    "sys.path.append(os.path.join(current_dir, '..', 'src'))\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from optimize import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../Data/'\n",
    "\n",
    "features_file_1 = 'featureSelection/selection_cleanDragonDescriptors.csv'\n",
    "features_file_3 = 'featureSelection/selection_cleanMordredDescriptors.csv'\n",
    "\n",
    "features_file_2 =  'deepnose_features.npy'\n",
    "CID_file = 'molecules_train_cid.npy'\n",
    "\n",
    "# Read all copies, before and after correction; before was also downloaded from Dropbox.\n",
    "mixture_file = 'Mixure_Definitions_Training_set.csv' \n",
    "training_task_file = 'TrainingData_mixturedist.csv'\n",
    "\n",
    "# Mordred features\n",
    "features_1 = pd.read_csv(os.path.join(input_path, features_file_1), index_col= 0)\n",
    "features_3 = pd.read_csv(os.path.join(input_path, features_file_3), index_col= 0)\n",
    "\n",
    "features_2 = np.load(os.path.join(input_path, features_file_2))\n",
    "\n",
    "features_CIDs = np.load(os.path.join(input_path, CID_file))\n",
    "# Training dataframe\n",
    "training_set = pd.read_csv(os.path.join(input_path, training_task_file))\n",
    "\n",
    "# Mapping helper files\n",
    "mixtures_IDs = pd.read_csv(os.path.join(input_path, mixture_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared = set(features_1.index.tolist()) & set(features_CIDs)\n",
    "len(shared) # this is expected!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_features_1_names = features_1_normalized.columns.tolist()\n",
    "# features_1_names = features_1.columns.tolist()\n",
    "# mordred_features_combined = list(set(normalized_features_1_names + features_1_names))\n",
    "# np.save('../Data/featureSelection/combined_dragon_feature_names.npy', mordred_features_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "# standardize Mordred\n",
    "features_1_np = scaler.fit_transform(features_1)\n",
    "features_1 = pd.DataFrame(features_1_np, columns=features_1.columns, index=features_1.index)\n",
    "\n",
    "features_3_np = scaler.fit_transform(features_3)\n",
    "features_3 = pd.DataFrame(features_3_np, columns=features_3.columns, index=features_3.index)\n",
    "\n",
    "# log standardize deepnose\n",
    "epsilon = 1e-8 \n",
    "features_2 = scaler.fit_transform(np.log(features_2 + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check the number of unique non-NaN values in each feature column\n",
    "num_unique_values = np.count_nonzero(~np.isnan(features_1), axis=0)\n",
    "\n",
    "# Print if the number of unique non-NaN values for each feature\n",
    "for i, count in enumerate(num_unique_values):\n",
    "    if count == 0:\n",
    "        print(f\"Feature {i}: {count} unique non-NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map CID to features:\n",
    "CID2features_dragon = {CID: np.array(features_1.loc[CID].tolist()) if CID in features_1.index else np.full(len(features_1.columns), np.nan) for CID in features_CIDs}\n",
    "CID2features_deepnose=  {CID: features_2[i] for i, CID in enumerate(features_CIDs)}\n",
    "CID2features_mordred =  {CID: features_3.loc[CID].tolist() for CID in features_CIDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m, y, num_mixtures, all_pairs_CIDs = format_Xy(training_set,  mixtures_IDs, CID2features_mordred, method = 'avg')\n",
    "X_dr, _, _, _  = format_Xy(training_set,  mixtures_IDs, CID2features_dragon, method = 'avg')\n",
    "X_d, _, _, _ = format_Xy(training_set,  mixtures_IDs, CID2features_deepnose, method = 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input pairs to a suitable format for training\n",
    "X_pairs_m = np.array([(np.concatenate((x1, x2))) for x1, x2 in X_m])\n",
    "X_pairs_d = np.array([(np.concatenate((x1, x2))) for x1, x2 in X_d])\n",
    "X_pairs_dr = np.array([(np.concatenate((x1, x2))) for x1, x2 in X_dr])\n",
    "\n",
    "y_true = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_m = [get_euclidean_distance(m[0], m[1]) for m in X_m]\n",
    "similarities_m = [get_cosine_similarity(m[0], m[1]) for m in X_m]\n",
    "angles_m = [get_cosine_angle(m[0], m[1]) for m in X_m] \n",
    "\n",
    "distances_d = [get_euclidean_distance(m[0], m[1]) for m in X_d]\n",
    "similarities_d = [get_cosine_similarity(m[0], m[1]) for m in X_d]\n",
    "angles_d = [get_cosine_angle(m[0], m[1]) for m in X_d] \n",
    "\n",
    "distances_dr = [get_euclidean_distance(m[0], m[1]) for m in X_dr]\n",
    "similarities_dr = [get_cosine_similarity(m[0], m[1]) for m in X_dr]\n",
    "angles_dr = [get_cosine_angle(m[0], m[1]) for m in X_dr] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_monos = [ len( set(pair[0]).intersection(set(pair[1]))) for pair in all_pairs_CIDs]\n",
    "diff_monos = [ len( set(pair[0]).difference(set(pair[1]))) for pair in all_pairs_CIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['Sum num monos'] = np.array(num_mixtures).sum(axis = 1)\n",
    "training_set['Shared'] = shared_monos\n",
    "training_set['Diff'] = diff_monos\n",
    "training_set['Num mixture1'] = np.array(num_mixtures)[:, 0]\n",
    "training_set['Num mixture2'] = np.array(num_mixtures)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = training_set['Dataset'].to_numpy()\n",
    "encoder = OneHotEncoder()\n",
    "data_arr = encoder.fit_transform(datasets.reshape(-1, 1))\n",
    "data_arr = data_arr.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add all information above\n",
    "# X_features = np.hstack( (X_pairs_d, X_pairs_m,\n",
    "#                         np.array(distances_m).reshape(500, 1), \n",
    "#                         np.array(similarities_m).reshape(500, 1), \n",
    "#                         np.array(angles_m).reshape(500, 1), \n",
    "#                         np.array(distances_d).reshape(500, 1), \n",
    "#                         np.array(similarities_d).reshape(500, 1), \n",
    "#                         np.array(angles_d).reshape(500, 1), \n",
    "#                         np.array(shared_monos).reshape(500, 1), \n",
    "#                         np.array(diff_monos).reshape(500, 1), \n",
    "#                         np.array(num_mixtures).reshape(500,2), \n",
    "#                         data_arr))\n",
    "X_features = np.hstack( (X_pairs_m, X_pairs_d, X_pairs_dr,\n",
    "                        np.array(distances_m).reshape(500, 1), \n",
    "                        np.array(similarities_m).reshape(500, 1), \n",
    "                        np.array(angles_m).reshape(500, 1), \n",
    "                        np.array(distances_d).reshape(500, 1), \n",
    "                        np.array(similarities_d).reshape(500, 1), \n",
    "                        np.array(angles_d).reshape(500, 1), \n",
    "                        np.array(distances_dr).reshape(500, 1), \n",
    "                        np.array(similarities_dr).reshape(500, 1), \n",
    "                        np.array(angles_dr).reshape(500, 1), \n",
    "                        np.array(shared_monos).reshape(500, 1), \n",
    "                        np.array(diff_monos).reshape(500, 1), \n",
    "                        np.array(num_mixtures).reshape(500,2), \n",
    "                        data_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R (Deepnose embedding Eucledian distance v.s Experimental Value):  0.4777691715913296\n"
     ]
    }
   ],
   "source": [
    "# dist_corr = np.corrcoef(distances_d, y_true)[0, 1]\n",
    "dist_corr = np.corrcoef(distances_m, y_true)[0, 1]\n",
    "\n",
    "print('R (Deepnose embedding Eucledian distance v.s Experimental Value): ', dist_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R (Cosyne similarity v.s. Experimental Value):  -0.4930930966753656\n"
     ]
    }
   ],
   "source": [
    "# sim_corr = np.corrcoef(similarities_d, y_true)[0, 1]\n",
    "sim_corr = np.corrcoef(similarities_m, y_true)[0, 1]\n",
    "\n",
    "print('R (Cosyne similarity v.s. Experimental Value): ', sim_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R (Vector angle v.s. Experimental Value):  0.5028727737317864\n"
     ]
    }
   ],
   "source": [
    "# sim_corr = np.corrcoef(angles_d, y_true)[0, 1]\n",
    "sim_corr = np.corrcoef(angles_m, y_true)[0, 1]\n",
    "\n",
    "print('R (Vector angle v.s. Experimental Value): ', sim_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "seed = 314159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_list = []\n",
    "xgb_pred_list = []\n",
    "kf_rf_importances = []\n",
    "y_true_list = []\n",
    "test_indices_list = []  # Keep track of the test indices in each fold\n",
    "\n",
    "# Perform k-fold cross-validation:\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "for train_index, test_index in kf.split(X_features):\n",
    "    X_train, X_test = X_features[train_index], X_features[test_index]\n",
    "    y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "    \n",
    "    # Train the Random Forest regressor\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=seed)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Train the XGBoost regressor\n",
    "    xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=seed)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions \n",
    "    rf_pred = rf.predict(X_test)\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Get the feature importances\n",
    "    importances = rf.feature_importances_\n",
    "    kf_rf_importances.append(importances)\n",
    "    rf_pred_list.extend(rf_pred)\n",
    "    xgb_pred_list.extend(xgb_pred)\n",
    "    y_true_list.extend(y_test)\n",
    "    test_indices_list.extend(test_index)  # Store the test indices\n",
    "\n",
    "# Store the predictions and actual values\n",
    "results_df = pd.DataFrame({\n",
    "    'test_index': test_indices_list,\n",
    "    'rf_pred': rf_pred_list,\n",
    "    'xgb_pred': xgb_pred_list,\n",
    "    'y_true': y_true_list\n",
    "})\n",
    "\n",
    "# Merge the results with the training_set df\n",
    "training_set = training_set.merge(results_df, left_index=True, right_on='test_index')\n",
    "training_set.drop('test_index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Mixture 1</th>\n",
       "      <th>Mixture 2</th>\n",
       "      <th>Experimental Values</th>\n",
       "      <th>Sum num monos</th>\n",
       "      <th>Shared</th>\n",
       "      <th>Diff</th>\n",
       "      <th>Num mixture1</th>\n",
       "      <th>Num mixture2</th>\n",
       "      <th>rf_pred</th>\n",
       "      <th>xgb_pred</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Snitz 1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.577419</td>\n",
       "      <td>0.555801</td>\n",
       "      <td>0.604167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Snitz 1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.651042</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.633379</td>\n",
       "      <td>0.718815</td>\n",
       "      <td>0.651042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Snitz 1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.505208</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.535816</td>\n",
       "      <td>0.411010</td>\n",
       "      <td>0.505208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Snitz 1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.411458</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>0.536816</td>\n",
       "      <td>0.452299</td>\n",
       "      <td>0.411458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Snitz 1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.588263</td>\n",
       "      <td>0.594626</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset  Mixture 1  Mixture 2  Experimental Values  Sum num monos  \\\n",
       "150  Snitz 1          1          2             0.604167             20   \n",
       "300  Snitz 1          1          3             0.651042             11   \n",
       "0    Snitz 1          1          5             0.505208             40   \n",
       "1    Snitz 1          1          6             0.411458             50   \n",
       "50   Snitz 1          1          7             0.562500             14   \n",
       "\n",
       "     Shared  Diff  Num mixture1  Num mixture2   rf_pred  xgb_pred    y_true  \n",
       "150       0    10            10            10  0.577419  0.555801  0.604167  \n",
       "300       0    10            10             1  0.633379  0.718815  0.651042  \n",
       "0         0    10            10            30  0.535816  0.411010  0.505208  \n",
       "1         0    10            10            40  0.536816  0.452299  0.411458  \n",
       "50        0    10            10             4  0.588263  0.594626  0.562500  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - R: 0.610\n",
      "Random Forest - RMSE: 0.124\n",
      "\n",
      "XGBoost - R: 0.579\n",
      "XGBoost - RMSE: 0.129\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation and R^2 for Random Forest\n",
    "rf_corr = np.corrcoef(rf_pred_list, y_true_list)[0, 1]\n",
    "rf_rmse = np.sqrt(mean_squared_error(np.array(y_true_list), np.array(rf_pred_list)))\n",
    "\n",
    "print(f\"Random Forest - R: {rf_corr:.3f}\")\n",
    "print(f\"Random Forest - RMSE: {rf_rmse:.3f}\")\n",
    "print()\n",
    "# Calculate the correlation and R^2 for XGBoost\n",
    "xgb_corr = np.corrcoef(xgb_pred_list, y_true_list)[0, 1]\n",
    "xgb_rmse = np.sqrt(mean_squared_error(np.array(y_true_list), np.array(xgb_pred_list)))\n",
    "\n",
    "print(f\"XGBoost - R: {xgb_corr:.3f}\")\n",
    "print(f\"XGBoost - RMSE: {xgb_rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting round : 0\n"
     ]
    }
   ],
   "source": [
    "for seed in [0, 1, 2]:\n",
    "    print(\"Starting round :\", seed)\n",
    "    rf_best, rgb_best = para_search(seed, X_features, y_true)\n",
    "    _ =  avg_rf_best(rf_best, X_features, y_true)\n",
    "    _ =  avg_rgb_best(rgb_best, X_features, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feedback",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
