{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Add the ./src folder to the Python module search path\n",
    "sys.path.append(os.path.join(current_dir, '..', 'src'))\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from optimize import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../Data/'\n",
    "\n",
    "features_file_1 = 'featureSelection/selection_cleanMordredDescriptors.csv'\n",
    "\n",
    "features_file_2 =  'deepnose_features.npy'\n",
    "CID_file = 'molecules_train_cid.npy'\n",
    "\n",
    "# Read all copies, before and after correction; before was also downloaded from Dropbox.\n",
    "mixture_file = 'Mixure_Definitions_Training_set.csv' \n",
    "training_task_file = 'TrainingData_mixturedist.csv'\n",
    "\n",
    "# Mordred features\n",
    "features_1 = pd.read_csv(os.path.join(input_path, features_file_1), index_col= 0)\n",
    "\n",
    "features_2 = np.load(os.path.join(input_path, features_file_2))\n",
    "\n",
    "features_CIDs = np.load(os.path.join(input_path, CID_file))\n",
    "# Training dataframe\n",
    "training_set = pd.read_csv(os.path.join(input_path, training_task_file))\n",
    "\n",
    "# Mapping helper files\n",
    "mixtures_IDs = pd.read_csv(os.path.join(input_path, mixture_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared = set(features_1.index.tolist()) & set(features_CIDs)\n",
    "len(shared) # this is expected!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_features_1_names = features_1_normalized.columns.tolist()\n",
    "# features_1_names = features_1.columns.tolist()\n",
    "# mordred_features_combined = list(set(normalized_features_1_names + features_1_names))\n",
    "# np.save('../Data/featureSelection/combined_dragon_feature_names.npy', mordred_features_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "# standardize Mordred\n",
    "features_1_np = scaler.fit_transform(features_1)\n",
    "features_1 = pd.DataFrame(features_1_np, columns=features_1.columns, index=features_1.index)\n",
    "\n",
    "\n",
    "# log standardize deepnose\n",
    "epsilon = 1e-8 \n",
    "features_2 = scaler.fit_transform(np.log(features_2 + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check the number of unique non-NaN values in each feature column\n",
    "num_unique_values = np.count_nonzero(~np.isnan(features_1), axis=0)\n",
    "\n",
    "# Print if the number of unique non-NaN values for each feature\n",
    "for i, count in enumerate(num_unique_values):\n",
    "    if count == 0:\n",
    "        print(f\"Feature {i}: {count} unique non-NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map CID to features:\n",
    "CID2features_deepnose=  {CID: features_2[i] for i, CID in enumerate(features_CIDs)}\n",
    "CID2features_mordred =  {CID: features_1.loc[CID].tolist() for CID in features_CIDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m, y, num_mixtures, all_pairs_CIDs = format_Xy(training_set,  mixtures_IDs, CID2features_mordred, method = 'avg')\n",
    "X_d, _, _, _ = format_Xy(training_set,  mixtures_IDs, CID2features_deepnose, method = 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input pairs to a suitable format for training\n",
    "X_pairs_m = np.array([(np.concatenate((x1, x2))) for x1, x2 in X_m])\n",
    "X_pairs_d = np.array([(np.concatenate((x1, x2))) for x1, x2 in X_d])\n",
    "\n",
    "y_true = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_m = [get_euclidean_distance(m[0], m[1]) for m in X_m]\n",
    "similarities_m = [get_cosine_similarity(m[0], m[1]) for m in X_m]\n",
    "angles_m = [get_cosine_angle(m[0], m[1]) for m in X_m] \n",
    "\n",
    "distances_d = [get_euclidean_distance(m[0], m[1]) for m in X_d]\n",
    "similarities_d = [get_cosine_similarity(m[0], m[1]) for m in X_d]\n",
    "angles_d = [get_cosine_angle(m[0], m[1]) for m in X_d] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_monos = [ len( set(pair[0]).intersection(set(pair[1]))) for pair in all_pairs_CIDs]\n",
    "diff_monos = [ len( set(pair[0]).difference(set(pair[1]))) for pair in all_pairs_CIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['Sum num monos'] = np.array(num_mixtures).sum(axis = 1)\n",
    "training_set['Shared'] = shared_monos\n",
    "training_set['Diff'] = diff_monos\n",
    "training_set['Num mixture1'] = np.array(num_mixtures)[:, 0]\n",
    "training_set['Num mixture2'] = np.array(num_mixtures)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = training_set['Dataset'].to_numpy()\n",
    "# Returns the uniques in the order of appearance\n",
    "desired_order = training_set['Dataset'].unique().tolist() \n",
    "encoder = OneHotEncoder(categories=[desired_order])\n",
    "data_arr = encoder.fit_transform(datasets.reshape(-1, 1))\n",
    "data_arr = data_arr.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = np.hstack( (X_pairs_m, X_pairs_d,\n",
    "                        np.array(distances_m).reshape(500, 1), \n",
    "                        np.array(similarities_m).reshape(500, 1), \n",
    "                        np.array(angles_m).reshape(500, 1), \n",
    "                        np.array(distances_d).reshape(500, 1), \n",
    "                        np.array(similarities_d).reshape(500, 1), \n",
    "                        np.array(angles_d).reshape(500, 1), \n",
    "                        np.array(shared_monos).reshape(500, 1), \n",
    "                        np.array(diff_monos).reshape(500, 1), \n",
    "                        np.array(num_mixtures).reshape(500,2), \n",
    "                        data_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R (Deepnose embedding Eucledian distance v.s Experimental Value):  0.4777691715913296\n"
     ]
    }
   ],
   "source": [
    "# dist_corr = np.corrcoef(distances_d, y_true)[0, 1]\n",
    "dist_corr = np.corrcoef(distances_m, y_true)[0, 1]\n",
    "\n",
    "print('R (Deepnose embedding Eucledian distance v.s Experimental Value): ', dist_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R (Cosyne similarity v.s. Experimental Value):  -0.4930930966753656\n"
     ]
    }
   ],
   "source": [
    "# sim_corr = np.corrcoef(similarities_d, y_true)[0, 1]\n",
    "sim_corr = np.corrcoef(similarities_m, y_true)[0, 1]\n",
    "\n",
    "print('R (Cosyne similarity v.s. Experimental Value): ', sim_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R (Vector angle v.s. Experimental Value):  0.5028727737317864\n"
     ]
    }
   ],
   "source": [
    "# sim_corr = np.corrcoef(angles_d, y_true)[0, 1]\n",
    "sim_corr = np.corrcoef(angles_m, y_true)[0, 1]\n",
    "\n",
    "print('R (Vector angle v.s. Experimental Value): ', sim_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with optimizing threshold function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "seed = 314159\n",
    "\n",
    "best_rf = {'n_estimators': 500, 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': True}\n",
    "best_rgb = {'subsample': 0.7, 'n_estimators': 400, 'max_depth': 9, 'learning_rate': 0.01, 'colsample_bytree': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_predictions(xgb_pred, rf_pred, threshold_low, threshold_high):\n",
    "    combined = np.where(xgb_pred <= threshold_low, xgb_pred,\n",
    "                        np.where(xgb_pred >= threshold_high, xgb_pred,\n",
    "                                 rf_pred))\n",
    "    return combined\n",
    "\n",
    "def objective_function(thresholds, xgb_pred, rf_pred, true_values):\n",
    "    combined_pred = combine_predictions(xgb_pred, rf_pred, thresholds[0], thresholds[1])\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, combined_pred))\n",
    "    corr = np.corrcoef(combined_pred, true_values)[0, 1]\n",
    "    return -corr * 0.1 + rmse * 0.9  # Adjust weights as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_pred_list = []\n",
    "# xgb_pred_list = []\n",
    "# kf_rf_importances = []\n",
    "# y_true_list = []\n",
    "# test_indices_list = []  # Keep track of the test indices in each fold\n",
    "\n",
    "# # Perform k-fold cross-validation:\n",
    "# kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "# for train_index, test_index in kf.split(X_features):\n",
    "\n",
    "#     X_train, X_test = X_features[train_index], X_features[test_index]\n",
    "#     y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "    \n",
    "#     # Train the Random Forest regressor\n",
    "#     rf_model = RandomForestRegressor(**best_rf, random_state=seed)\n",
    "#     rf_model.fit(X_train, y_train)\n",
    "    \n",
    "#     # Train the XGBoost regressor\n",
    "#     xgb_model = xgb.XGBRegressor(**best_rgb, random_state=seed)\n",
    "#     xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "#     # Make predictions \n",
    "#     rf_pred = rf_model.predict(X_test)\n",
    "#     xgb_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "#     # Get the feature importances\n",
    "#     importances = rf_model.feature_importances_\n",
    "#     kf_rf_importances.append(importances)\n",
    "#     rf_pred_list.extend(rf_pred)\n",
    "#     xgb_pred_list.extend(xgb_pred)\n",
    "#     y_true_list.extend(y_test)\n",
    "#     test_indices_list.extend(test_index)  # Store the test indices\n",
    "\n",
    "# # Store the predictions and actual values\n",
    "# results_df = pd.DataFrame({\n",
    "#     'test_index': test_indices_list,\n",
    "#     'rf_pred': rf_pred_list,\n",
    "#     'xgb_pred': xgb_pred_list,\n",
    "#     'y_true': y_true_list\n",
    "# })\n",
    "\n",
    "# # Merge the results with the training_set df\n",
    "# training_set = training_set.merge(results_df, left_index=True, right_on='test_index')\n",
    "# training_set.drop('test_index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the correlation and R^2 for Random Forest\n",
    "# rf_corr = np.corrcoef(rf_pred_list, y_true_list)[0, 1]\n",
    "# rf_rmse = np.sqrt(mean_squared_error(np.array(y_true_list), np.array(rf_pred_list)))\n",
    "\n",
    "# print(f\"Random Forest - R: {rf_corr:.3f}\")\n",
    "# print(f\"Random Forest - RMSE: {rf_rmse:.3f}\")\n",
    "# print()\n",
    "# # Calculate the correlation and R^2 for XGBoost\n",
    "# xgb_corr = np.corrcoef(xgb_pred_list, y_true_list)[0, 1]\n",
    "# xgb_rmse = np.sqrt(mean_squared_error(np.array(y_true_list), np.array(xgb_pred_list)))\n",
    "\n",
    "# print(f\"XGBoost - R: {xgb_corr:.3f}\")\n",
    "# print(f\"XGBoost - RMSE: {xgb_rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_list = []\n",
    "xgb_pred_list = []\n",
    "combined_pred_list = []\n",
    "kf_rf_importances = []\n",
    "y_true_list = []\n",
    "test_indices_list = []\n",
    "optimal_thresholds_list = []\n",
    "\n",
    "# Perform k-fold cross-validation:\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "for train_index, test_index in kf.split(X_features):\n",
    "\n",
    "    X_train, X_test = X_features[train_index], X_features[test_index]\n",
    "    y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "    \n",
    "    # Train the Random Forest regressor\n",
    "    rf_model = RandomForestRegressor(**best_rf, random_state=seed)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Train the XGBoost regressor\n",
    "    xgb_model = xgb.XGBRegressor(**best_rgb, random_state=seed)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions \n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Optimize thresholds\n",
    "    initial_thresholds = [0.4, 0.6]  # Starting guess\n",
    "    # initial_thresholds = [0.3, 0.7]  # Starting guess\n",
    "\n",
    "    result = minimize(lambda x: objective_function(x, xgb_pred, rf_pred, y_test),\n",
    "                      initial_thresholds,\n",
    "                      method='Nelder-Mead',\n",
    "                      bounds=[(0, 1), (0, 1)])\n",
    "    \n",
    "    optimal_thresholds = result.x\n",
    "    optimal_thresholds_list.append(optimal_thresholds)\n",
    "    \n",
    "    # Generate combined predictions\n",
    "    combined_pred = combine_predictions(xgb_pred, rf_pred, optimal_thresholds[0], optimal_thresholds[1])\n",
    "    \n",
    "    # Get the feature importances\n",
    "    importances = rf_model.feature_importances_\n",
    "    kf_rf_importances.append(importances)\n",
    "    \n",
    "    rf_pred_list.extend(rf_pred)\n",
    "    xgb_pred_list.extend(xgb_pred)\n",
    "    combined_pred_list.extend(combined_pred)\n",
    "    y_true_list.extend(y_test)\n",
    "    test_indices_list.extend(test_index)\n",
    "\n",
    "# Store the predictions and actual values\n",
    "results_df = pd.DataFrame({\n",
    "    'test_index': test_indices_list,\n",
    "    'rf_pred': rf_pred_list,\n",
    "    'xgb_pred': xgb_pred_list,\n",
    "    'combined_pred': combined_pred_list,\n",
    "    'y_true': y_true_list\n",
    "})\n",
    "\n",
    "# Merge the results with the training_set df\n",
    "training_set = training_set.merge(results_df, left_index=True, right_on='test_index')\n",
    "training_set.drop('test_index', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF RMSE: 0.12197581633078773 \n",
      "Correlation: 0.6474515913228636\n",
      "XGB RMSE: 0.12131653217126667 \n",
      "Correlation: 0.633644940861654\n",
      "Combined RMSE: 0.1193005272566889 \n",
      "Correlation: 0.6552910046267708\n",
      "Average optimal thresholds: [0.40566357 0.61836108]\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall performance metrics\n",
    "overall_rf_rmse = np.sqrt(mean_squared_error(y_true_list, rf_pred_list))\n",
    "overall_xgb_rmse = np.sqrt(mean_squared_error(y_true_list, xgb_pred_list))\n",
    "overall_combined_rmse = np.sqrt(mean_squared_error(y_true_list, combined_pred_list))\n",
    "\n",
    "overall_rf_corr = np.corrcoef(rf_pred_list, y_true_list)[0, 1]\n",
    "overall_xgb_corr = np.corrcoef(xgb_pred_list, y_true_list)[0, 1]\n",
    "overall_combined_corr = np.corrcoef(combined_pred_list, y_true_list)[0, 1]\n",
    "\n",
    "print(f\"RF RMSE: {overall_rf_rmse} \\nCorrelation: {overall_rf_corr}\\n\")\n",
    "print(f\"XGB RMSE: {overall_xgb_rmse} \\nCorrelation: {overall_xgb_corr}\\n\")\n",
    "print(f\"Combined RMSE: {overall_combined_rmse} \\nCorrelation: {overall_combined_corr}\\n\")\n",
    "\n",
    "# Average optimal thresholds\n",
    "avg_optimal_thresholds = np.mean(optimal_thresholds_list, axis=0)\n",
    "print(f\"Average optimal thresholds: {avg_optimal_thresholds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final models on full data\n",
    "# final_rf_model = RandomForestRegressor(**best_rf, random_state=seed)\n",
    "# final_rf_model.fit(X_features, y_true)\n",
    "# final_xgb_model = xgb.XGBRegressor(**best_rgb, random_state=seed)\n",
    "# final_xgb_model.fit(X_features, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_combined_predictions(X):\n",
    "#     rf_pred = final_rf_model.predict(X)\n",
    "#     xgb_pred = final_xgb_model.predict(X)\n",
    "#     combined_pred = combine_predictions(xgb_pred, rf_pred, avg_optimal_thresholds[0], avg_optimal_thresholds[1])\n",
    "#     return combined_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function for your leaderboard submissions\n",
    "# leaderboard_predictions = make_combined_predictions(X_leaderboard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feedback",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
