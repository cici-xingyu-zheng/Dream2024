{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Add the ./src folder to the Python module search path\n",
    "sys.path.append(os.path.join(current_dir, '..', 'src'))\n",
    "\n",
    "from train_test import *\n",
    "from utils import *\n",
    "from optimize import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Ridge \n",
    "import xgboost as xgb\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../Data/'\n",
    "\n",
    "features_file_1 = 'featureSelection/selection_cleanMordredDescriptors.csv'\n",
    "features_file_2 =  'deepnose_features.npy'\n",
    "CID_file = 'molecules_train_cid.npy'\n",
    "\n",
    "# Read all copies, before and after correction; before was also downloaded from Dropbox.\n",
    "mixture_file = 'Mixure_Definitions_Training_set_UPD2.csv' \n",
    "training_task_file = 'TrainingData_mixturedist.csv'\n",
    "\n",
    "# Mordred features\n",
    "features_1 = pd.read_csv(os.path.join(input_path, features_file_1), index_col= 0)\n",
    "\n",
    "features_2 = np.load(os.path.join(input_path, features_file_2))\n",
    "\n",
    "features_CIDs = np.load(os.path.join(input_path, CID_file))\n",
    "# Training dataframe\n",
    "training_set = pd.read_csv(os.path.join(input_path, training_task_file))\n",
    "\n",
    "# Mapping helper files\n",
    "mixtures_IDs = pd.read_csv(os.path.join(input_path, mixture_file))\n",
    "\n",
    "\n",
    "feature_file_3 = 'Fingerprints/Morgan_Fingerprints_Frequency_Size50.csv'\n",
    "features_3 = pd.read_csv(os.path.join(input_path, feature_file_3), index_col= 0)\n",
    "features_file_4 =  'leffingwell_features_96.npy'\n",
    "features_4 = np.load(os.path.join(input_path, features_file_4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "# standardize Mordred\n",
    "features_1_np = scaler.fit_transform(features_1)\n",
    "features_1 = pd.DataFrame(features_1_np, columns=features_1.columns, index=features_1.index)\n",
    "\n",
    "\n",
    "# log standardize deepnose\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "epsilon = 1e-8 \n",
    "features_2 = scaler.fit_transform(np.log(features_2 + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check the number of unique non-NaN values in each feature column\n",
    "num_unique_values = np.count_nonzero(~np.isnan(features_1), axis=0)\n",
    "\n",
    "# Print if the number of unique non-NaN values for each feature\n",
    "for i, count in enumerate(num_unique_values):\n",
    "    if count == 0:\n",
    "        print(f\"Feature {i}: {count} unique non-NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map CID to features:\n",
    "CID2features_deepnose=  {CID: features_2[i] for i, CID in enumerate(features_CIDs)}\n",
    "CID2features_mordred =  {CID: features_1.loc[CID].tolist() for CID in features_CIDs}\n",
    "CID2features_morgan =  {CID: features_3.loc[CID].tolist() for CID in features_CIDs}\n",
    "CID2features_leffingwell = {CID: features_4[i] for i, CID in enumerate(features_CIDs)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m, y, num_mixtures, all_pairs_CIDs = format_Xy(training_set,  mixtures_IDs, CID2features_mordred, method = 'avg')\n",
    "X_d, _, _, _ = format_Xy(training_set,  mixtures_IDs, CID2features_deepnose, method = 'avg')\n",
    "X_mg, y, _, _ = format_Xy(training_set,  mixtures_IDs, CID2features_morgan, method = 'sum')\n",
    "X_lw, y, _, _ = format_Xy(training_set,  mixtures_IDs, CID2features_leffingwell, method = 'sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input pairs to a suitable format for training\n",
    "X_pairs_m = np.array([(np.concatenate((x1, x2))) for x1, x2 in X_m])\n",
    "X_pairs_d = np.array([(np.concatenate((x1, x2))) for x1, x2 in X_d])\n",
    "X_pairs_mg= np.array([(np.concatenate((x1, x2))) for x1, x2 in X_mg])\n",
    "X_pairs_lw= np.array([(np.concatenate((x1, x2))) for x1, x2 in X_lw])\n",
    "\n",
    "y_true = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_m = [get_euclidean_distance(m[0], m[1]) for m in X_m]\n",
    "similarities_m = [get_cosine_similarity(m[0], m[1]) for m in X_m]\n",
    "angles_m = [get_cosine_angle(m[0], m[1]) for m in X_m] \n",
    "\n",
    "distances_d = [get_euclidean_distance(m[0], m[1]) for m in X_d]\n",
    "similarities_d = [get_cosine_similarity(m[0], m[1]) for m in X_d]\n",
    "angles_d = [get_cosine_angle(m[0], m[1]) for m in X_d] \n",
    "\n",
    "distances_mg = [get_euclidean_distance(m[0], m[1]) for m in X_mg]\n",
    "similarities_mg = [get_cosine_similarity(m[0], m[1]) for m in X_mg]\n",
    "angles_mg = [get_cosine_angle(m[0], m[1]) for m in X_mg] \n",
    "\n",
    "distances_lw = [get_euclidean_distance(m[0], m[1]) for m in X_lw]\n",
    "similarities_lw = [get_cosine_similarity(m[0], m[1]) for m in X_lw]\n",
    "angles_lw = [get_cosine_angle(m[0], m[1]) for m in X_lw] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_monos = [ len( set(pair[0]).intersection(set(pair[1]))) for pair in all_pairs_CIDs]\n",
    "diff_monos = [ len( set(pair[0]).difference(set(pair[1]))) for pair in all_pairs_CIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['Sum num monos'] = np.array(num_mixtures).sum(axis = 1)\n",
    "training_set['Shared'] = shared_monos\n",
    "training_set['Diff'] = diff_monos\n",
    "training_set['Num mixture1'] = np.array(num_mixtures)[:, 0]\n",
    "training_set['Num mixture2'] = np.array(num_mixtures)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = training_set['Dataset'].to_numpy()\n",
    "# Returns the uniques in the order of appearance\n",
    "desired_order = training_set['Dataset'].unique().tolist() \n",
    "encoder = OneHotEncoder(categories=[desired_order])\n",
    "data_arr = encoder.fit_transform(datasets.reshape(-1, 1))\n",
    "data_arr = data_arr.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Snitz 1', 'Snitz 2', 'Ravia', 'Bushdid']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dense = np.hstack( (  X_pairs_m, \n",
    "                        np.array(distances_m).reshape(500, 1), \n",
    "                        np.array(similarities_m).reshape(500, 1), \n",
    "                        np.array(angles_m).reshape(500, 1), \n",
    "                        X_pairs_d,\n",
    "                        np.array(distances_d).reshape(500, 1), \n",
    "                        np.array(similarities_d).reshape(500, 1), \n",
    "                        np.array(angles_d).reshape(500, 1), \n",
    "                        np.array(shared_monos).reshape(500, 1), \n",
    "                        np.array(diff_monos).reshape(500, 1), \n",
    "                        np.array(num_mixtures).reshape(500,2), \n",
    "                        data_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sparse = np.hstack( (X_pairs_mg,\n",
    "                        np.array(distances_mg).reshape(500, 1), \n",
    "                        np.array(similarities_mg).reshape(500, 1), \n",
    "                        np.array(angles_mg).reshape(500, 1), \n",
    "                        X_pairs_lw,\n",
    "                        np.array(distances_lw).reshape(500, 1), \n",
    "                        np.array(similarities_lw).reshape(500, 1), \n",
    "                        np.array(angles_lw).reshape(500, 1),                        \n",
    "                        np.array(shared_monos).reshape(500, 1), \n",
    "                        np.array(diff_monos).reshape(500, 1), \n",
    "                        np.array(num_mixtures).reshape(500,2), \n",
    "                        data_arr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with optimizing threshold function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "seed = 314159\n",
    "\n",
    "best_rf_dense = {'n_estimators': 500, 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': True}\n",
    "best_rf_sparse = {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 0.5, 'max_depth': 30, 'bootstrap': True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation Performance:\n",
      "Dense Model Performance: {'RMSE': 0.12248608448322859, 'Correlation': 0.6414345447059586}\n",
      "Sparse Model Performance: {'RMSE': 0.12308167512831363, 'Correlation': 0.6183843395845805}\n",
      "Stacked Model Performance: {'RMSE': 0.11969892217196872, 'Correlation': 0.6448669668837788}\n"
     ]
    }
   ],
   "source": [
    "def stacking_ensemble_cv(X_dense, X_sparse, y, base_model_dense, base_model_sparse, meta_model, n_folds=10):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=314159)\n",
    "    \n",
    "    dense_preds = np.zeros(len(y))\n",
    "    sparse_preds = np.zeros(len(y))\n",
    "    meta_preds = np.zeros(len(y))\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_dense):\n",
    "        X_dense_train, X_dense_val = X_dense[train_index], X_dense[val_index]\n",
    "        X_sparse_train, X_sparse_val = X_sparse[train_index], X_sparse[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        # Train and predict with base models\n",
    "        base_model_dense.fit(X_dense_train, y_train)\n",
    "        base_model_sparse.fit(X_sparse_train, y_train)\n",
    "        \n",
    "        dense_preds[val_index] = base_model_dense.predict(X_dense_val)\n",
    "        sparse_preds[val_index] = base_model_sparse.predict(X_sparse_val)\n",
    "        \n",
    "        # Train and predict with meta model\n",
    "        meta_features_train = np.column_stack((\n",
    "            base_model_dense.predict(X_dense_train),\n",
    "            base_model_sparse.predict(X_sparse_train)\n",
    "        ))\n",
    "        meta_model.fit(meta_features_train, y_train)\n",
    "        \n",
    "        meta_features_val = np.column_stack((dense_preds[val_index], sparse_preds[val_index]))\n",
    "        meta_preds[val_index] = meta_model.predict(meta_features_val)\n",
    "    \n",
    "    # Evaluate models\n",
    "    dense_rmse = np.sqrt(mean_squared_error(y, dense_preds))\n",
    "    dense_corr, _ = pearsonr(y, dense_preds)\n",
    "    sparse_rmse = np.sqrt(mean_squared_error(y, sparse_preds))\n",
    "    sparse_corr, _ = pearsonr(y, sparse_preds)\n",
    "    stacked_rmse = np.sqrt(mean_squared_error(y, meta_preds))\n",
    "    stacked_corr, _ = pearsonr(y, meta_preds)\n",
    "    \n",
    "    return {\n",
    "        'performance': {\n",
    "            'dense_model': {'RMSE': dense_rmse, 'Correlation': dense_corr},\n",
    "            'sparse_model': {'RMSE': sparse_rmse, 'Correlation': sparse_corr},\n",
    "            'stacked_model': {'RMSE': stacked_rmse, 'Correlation': stacked_corr}\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Usage for cross-validation\n",
    "base_model_dense = RandomForestRegressor(**best_rf_dense, random_state=314159)\n",
    "base_model_sparse = RandomForestRegressor(**best_rf_sparse, random_state=314159)\n",
    "meta_model = Ridge()\n",
    "\n",
    "cv_results = stacking_ensemble_cv(X_dense, X_sparse, y_true, base_model_dense, base_model_sparse, meta_model)\n",
    "\n",
    "print(\"Cross-validation Performance:\")\n",
    "print(\"Dense Model Performance:\", cv_results['performance']['dense_model'])\n",
    "print(\"Sparse Model Performance:\", cv_results['performance']['sparse_model'])\n",
    "print(\"Stacked Model Performance:\", cv_results['performance']['stacked_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_models(X_dense, X_sparse, y, base_model_dense_class, base_model_sparse_class, meta_model_class, n_models=10):\n",
    "    final_models = []\n",
    "    \n",
    "    for seed in range(n_models):\n",
    "        base_model_dense = base_model_dense_class(**best_rf_dense, random_state=seed)\n",
    "        base_model_sparse = base_model_sparse_class(**best_rf_sparse, random_state=seed)\n",
    "        meta_model = meta_model_class()\n",
    "        \n",
    "        # Train base models\n",
    "        final_base_model_dense = base_model_dense.fit(X_dense, y)\n",
    "        final_base_model_sparse = base_model_sparse.fit(X_sparse, y)\n",
    "        \n",
    "        # Train meta model\n",
    "        final_meta_features = np.column_stack((\n",
    "            final_base_model_dense.predict(X_dense),\n",
    "            final_base_model_sparse.predict(X_sparse)\n",
    "        ))\n",
    "        final_meta_model = meta_model.fit(final_meta_features, y)\n",
    "        \n",
    "        final_models.append((final_base_model_dense, final_base_model_sparse, final_meta_model))\n",
    "    \n",
    "    return final_models\n",
    "\n",
    "\n",
    "def predict_stacked_ensemble(X_dense_new, X_sparse_new, final_models):\n",
    "    dense_predictions = []\n",
    "    sparse_predictions = []\n",
    "    meta_predictions = []\n",
    "    \n",
    "    for dense_model, sparse_model, meta_model in final_models:\n",
    "        dense_pred = dense_model.predict(X_dense_new)\n",
    "        sparse_pred = sparse_model.predict(X_sparse_new)\n",
    "        \n",
    "        dense_predictions.append(dense_pred)\n",
    "        sparse_predictions.append(sparse_pred)\n",
    "        \n",
    "        meta_features = np.column_stack((dense_pred, sparse_pred))\n",
    "        meta_pred = meta_model.predict(meta_features)\n",
    "        meta_predictions.append(meta_pred)\n",
    "    \n",
    "    mean_dense_pred = np.mean(dense_predictions, axis=0)\n",
    "    mean_sparse_pred = np.mean(sparse_predictions, axis=0)\n",
    "    mean_meta_pred = np.mean(meta_predictions, axis=0)\n",
    "    \n",
    "    return {\n",
    "        'dense_prediction': mean_dense_pred,\n",
    "        'sparse_prediction': mean_sparse_pred,\n",
    "        'meta_prediction': mean_meta_pred\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_models = train_final_models(X_dense, X_sparse, y_true, RandomForestRegressor, RandomForestRegressor, Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [CID2features_mordred, CID2features_deepnose]\n",
    "features_list_sparse = [CID2features_morgan, CID2features_leffingwell]\n",
    "X_dense_new, y_test_true = stacking_X_test_features(features_list,  X_dense, \"avg\")\n",
    "X_sparse_new, _ = stacking_X_test_features(features_list_sparse,  X_sparse, \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "predictions = predict_stacked_ensemble(X_dense_new, X_sparse_new, final_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Model Performance:\n",
      "  Correlation: 0.724\n",
      "  RMSE: 0.120\n",
      "\n",
      "Sparse Model Performance:\n",
      "  Correlation: 0.683\n",
      "  RMSE: 0.120\n",
      "\n",
      "Meta Model Performance:\n",
      "  Correlation: 0.714\n",
      "  RMSE: 0.116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_type in ['dense', 'sparse', 'meta']:\n",
    "    pred = predictions[f'{model_type}_prediction']\n",
    "    corr, _ = pearsonr(pred, y_test_true)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_true, pred))\n",
    "    \n",
    "    print(f\"{model_type.capitalize()} Model Performance:\")\n",
    "    print(f\"  Correlation: {corr:.3f}\")\n",
    "    print(f\"  RMSE: {rmse:.3f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feedback",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
