{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Add the ./src folder to the Python module search path\n",
    "sys.path.append(os.path.join(current_dir, '..', 'src'))\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../Data/'\n",
    "\n",
    "feature_file = 'Fingerprints/Morgan_Fingerprints_Frequency_Size50.csv'\n",
    "# feature_file = 'Fingerprints/TopologicalTorsions_Fingerprints_Frequency_Size50.csv'\n",
    "features_file_2 =  'leffingwell_features.npy'\n",
    "CID_file = 'molecules_train_cid.npy'\n",
    "\n",
    "# Read all copies, before and after correction; before was also downloaded from Dropbox.\n",
    "mixture_file = 'Mixure_Definitions_Training_set_UPD2.csv' \n",
    "training_task_file = 'TrainingData_mixturedist.csv'\n",
    "\n",
    "# Mordred features\n",
    "features_1 = pd.read_csv(os.path.join(input_path, feature_file), index_col= 0)\n",
    "features_2 = np.load(os.path.join(input_path, features_file_2))\n",
    "\n",
    "features_CIDs = np.load(os.path.join(input_path, CID_file))\n",
    "\n",
    "# Training dataframe\n",
    "training_set = pd.read_csv(os.path.join(input_path, training_task_file))\n",
    "\n",
    "# Mapping helper files\n",
    "mixtures_IDs = pd.read_csv(os.path.join(input_path, mixture_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map CID to 96 dim features:\n",
    "CID2features_morgan =  {CID: features_1.loc[CID].tolist() for CID in features_CIDs}\n",
    "CID2features_leffingwell = {CID: features_2[i] for i, CID in enumerate(features_CIDs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m, y, num_mixtures, all_pairs_CIDs = format_Xy(training_set,  mixtures_IDs, CID2features_morgan, method = 'sum')\n",
    "X_l, _, _, _ = format_Xy(training_set,  mixtures_IDs, CID2features_leffingwell, method = 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input pairs to a suitable format for training\n",
    "X_pairs_m = np.array([(np.concatenate((x1, x2))) for x1, x2 in X_m])\n",
    "X_pairs_l = np.array([(np.concatenate((x1, x2))) for x1, x2 in X_l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = X_pairs_l\n",
    "feature_dim = X_features.shape[1]\n",
    "\n",
    "y_true = np.array(y)\n",
    "\n",
    "# Feature names\n",
    "feature_names = [f'Lw_dim{i}_1' for i in range(113)] +  [f'Lw_dim{i}_2' for i in range(113)]\n",
    "name2index = dict(zip(feature_names, list(range(len(feature_names)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1th round reduction! \n",
      "\n",
      "Number of features used now: 226\n",
      "Random Forest - R: 0.442\n",
      "Random Forest - RMSE: 0.140\n",
      "\n",
      "Remove 34 of features;\n",
      "Left with 192 features.\n",
      "\n",
      "Starting 1th round reduction! \n",
      "\n",
      "Number of features used now: 192\n",
      "Random Forest - R: 0.447\n",
      "Random Forest - RMSE: 0.140\n",
      "\n",
      "Remove 0 of features;\n",
      "Left with 192 features.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the loop:\n",
    "n_folds = 10\n",
    "seed = 314159\n",
    "num_removal = 300\n",
    "iter = 0\n",
    "\n",
    "while num_removal != 0:\n",
    "    print(f'Starting {iter + 1}th round reduction! \\n')\n",
    "    print(f'Number of features used now: {feature_dim}')\n",
    "\n",
    "    rf_pred_list = []\n",
    "    y_true_list = []\n",
    "    low_importance_features = []\n",
    "    kf_importances = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    for train_index, test_index in kf.split(X_features):\n",
    "        X_train, X_test = X_features[train_index], X_features[test_index]\n",
    "        y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "        \n",
    "        # Train the Random Forest regressor\n",
    "        rf = RandomForestRegressor(n_estimators=100,\n",
    "                                    random_state=seed)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        rf_pred = rf.predict(X_test)\n",
    "        rf_pred_list.extend(rf_pred)\n",
    "        y_true_list.extend(y_test)\n",
    " \n",
    "        # Get the feature importances\n",
    "        importances = rf.feature_importances_\n",
    "        kf_importances.append(importances)\n",
    "        # Find the indices of features with importance < 0.00010\n",
    "        low_importance_indices_fold = np.where(importances < 0.00100)[0]\n",
    "\n",
    "        # Get the names of the low importance features\n",
    "        low_importance_features_fold = [feature_names[i] for i in low_importance_indices_fold]\n",
    "        # Append the low importance features for this fold to the list\n",
    "        low_importance_features.append(low_importance_features_fold)\n",
    "\n",
    "    # Calculate the correlation and R^2 for Random Forest\n",
    "    rf_corr = np.corrcoef(rf_pred_list, y_true_list)[0, 1]\n",
    "    rf_rmse = np.sqrt(mean_squared_error(np.array(y_true_list), np.array(rf_pred_list)))\n",
    "\n",
    "    print(f\"Random Forest - R: {rf_corr:.3f}\")\n",
    "    print(f\"Random Forest - RMSE: {rf_rmse:.3f}\")\n",
    "    print()\n",
    "\n",
    "    # Find the features that appear in all the low importance lists\n",
    "    consistent_low_importance_features = set.intersection(*map(set, low_importance_features))\n",
    "\n",
    "    # If the Descriptor shows up for both molecules, we select it:\n",
    "    feature_candidates = [c[:-2] for c in consistent_low_importance_features]\n",
    "    element_counts = Counter(feature_candidates)\n",
    "    descriptors_to_remove = [element for element, count in element_counts.items() if count == 2]\n",
    "\n",
    "    features_to_remove_1 = [f'{d}_1' for d in descriptors_to_remove]\n",
    "    features_to_remove_2 = [f'{d}_2' for d in descriptors_to_remove]\n",
    "\n",
    "    features_to_remove = features_to_remove_1 + features_to_remove_2\n",
    "    idx_to_remove = [name2index[f] for f in features_to_remove]\n",
    "    num_removal = len(features_to_remove)\n",
    "\n",
    "    print(f'Remove {num_removal} of features;')\n",
    "\n",
    "    feature_dim = feature_dim - len(features_to_remove)\n",
    "    print(f'Left with {feature_dim} features.')\n",
    "    print()\n",
    "\n",
    "    # Update feature matrix:\n",
    "    X_features = np.delete(X_features, idx_to_remove, axis=1)\n",
    "    feature_names = [feature for feature in feature_names if feature not in features_to_remove]\n",
    "    name2index = dict(zip(feature_names, list(range(len(feature_names)))))\n",
    "\n",
    "    # iter += 1\n",
    "    # np.save(f'Output/feature_reduction/feature_names_iter_{iter}.npy', feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_numbers = [int(re.search(r'dim(\\d+)_', name).group(1)) for name in feature_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = list(set(extracted_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 113)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2_kept = features_2[:, to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 96)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_2_kept.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
