{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Add the ./src folder to the Python module search path\n",
    "sys.path.append(os.path.join(current_dir, '..', 'src'))\n",
    "\n",
    "from utils import *\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../Data/'\n",
    "\n",
    "feature_file = 'Mordred_reduced_features_50.npy'\n",
    "features_file_2 =  'deepnose_features.npy'\n",
    "CID_file = 'molecules_train_cid.npy'\n",
    "\n",
    "# Read all copies, before and after correction; before was also downloaded from Dropbox.\n",
    "mixture_file = 'Mixure_Definitions_Training_set_UPD2.csv' \n",
    "\n",
    "training_task_file = 'TrainingData_mixturedist.csv'\n",
    "\n",
    "# Mordred features\n",
    "features = np.load(os.path.join(input_path, feature_file))\n",
    "features_2 = np.load(os.path.join(input_path, features_file_2))\n",
    "\n",
    "features_CIDs = np.load(os.path.join(input_path, CID_file))\n",
    "\n",
    "# Training dataframe\n",
    "training_set = pd.read_csv(os.path.join(input_path, training_task_file))\n",
    "\n",
    "# Mapping helper files\n",
    "mixtures_IDs = pd.read_csv(os.path.join(input_path, mixture_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "# standardize Mordred\n",
    "features = scaler.fit_transform(features)\n",
    "# log standardize deepnose\n",
    "epsilon = 1e-8 \n",
    "features_2 = scaler.fit_transform(np.log(features_2 + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to a numpy array\n",
    "features_array = features\n",
    "\n",
    "# Create an imputer object with mean strategy, can change later!!!\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "# Impute missing values\n",
    "imputed_features = imputer.fit_transform(features_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check the number of unique non-NaN values in each feature column\n",
    "num_unique_values = np.count_nonzero(~np.isnan(features), axis=0)\n",
    "\n",
    "# Print if the number of unique non-NaN values for each feature\n",
    "for i, count in enumerate(num_unique_values):\n",
    "    if count == 0:\n",
    "        print(f\"Feature {i}: {count} unique non-NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map CID to 96 dim features:\n",
    "CID2features_mordred =  {CID: imputed_features[i] for i, CID in enumerate(features_CIDs)}\n",
    "CID2features_deepnose=  {CID: features_2[i] for i, CID in enumerate(features_CIDs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m, y, num_mixtures, all_pairs_CIDs = format_Xy(training_set,  mixtures_IDs, CID2features_mordred, method = 'avg')\n",
    "X_d, _, _, _ = format_Xy(training_set,  mixtures_IDs, CID2features_deepnose, method = 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input pairs to a suitable format for training\n",
    "X_pairs_m = np.array([(np.concatenate((x1, x2))) for x1, x2 in X_m])\n",
    "X_pairs_d = np.array([(np.concatenate((x1, x2))) for x1, x2 in X_d])\n",
    "\n",
    "y_true = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_m = [get_euclidean_distance(m[0], m[1]) for m in X_m]\n",
    "similarities_m = [get_cosine_similarity(m[0], m[1]) for m in X_m]\n",
    "angles_m = [get_cosine_angle(m[0], m[1]) for m in X_m] \n",
    "\n",
    "distances_d = [get_euclidean_distance(m[0], m[1]) for m in X_d]\n",
    "similarities_d = [get_cosine_similarity(m[0], m[1]) for m in X_d]\n",
    "angles_d = [get_cosine_angle(m[0], m[1]) for m in X_d] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_monos = [ len( set(pair[0]).intersection(set(pair[1]))) for pair in all_pairs_CIDs]\n",
    "diff_monos = [ len( set(pair[0]).difference(set(pair[1]))) for pair in all_pairs_CIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['Sum num monos'] = np.array(num_mixtures).sum(axis = 1)\n",
    "training_set['Shared'] = shared_monos\n",
    "training_set['Diff'] = diff_monos\n",
    "training_set['Num mixture1'] = np.array(num_mixtures)[:, 0]\n",
    "training_set['Num mixture2'] = np.array(num_mixtures)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = training_set['Dataset'].to_numpy()\n",
    "encoder = OneHotEncoder()\n",
    "data_arr = encoder.fit_transform(datasets.reshape(-1, 1))\n",
    "data_arr = data_arr.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add all information above\n",
    "X_features = np.hstack( (X_pairs_d, X_pairs_m,\n",
    "                        np.array(distances_m).reshape(500, 1), \n",
    "                        np.array(similarities_m).reshape(500, 1), \n",
    "                        np.array(angles_m).reshape(500, 1), \n",
    "                        np.array(distances_d).reshape(500, 1), \n",
    "                        np.array(similarities_d).reshape(500, 1), \n",
    "                        np.array(angles_d).reshape(500, 1), \n",
    "                        np.array(shared_monos).reshape(500, 1), \n",
    "                        np.array(diff_monos).reshape(500, 1), \n",
    "                        np.array(num_mixtures).reshape(500,2), \n",
    "                        data_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 2. Train 10 models on all data, using previously found best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best =  {'n_estimators': 300, 'min_samples_split': 10, 'max_features': 'sqrt', 'max_depth': None, 'bootstrap': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 10 random forest models\n",
    "num_models = 10\n",
    "rf_models = []\n",
    "for i in range(num_models):\n",
    "    rf = RandomForestRegressor(**rf_best, random_state=i)\n",
    "    rf.fit(X_features, y_true)\n",
    "    rf_models.append(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. First testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_task_file = 'Data/LeaderboardData_mixturedist.csv'\n",
    "test_set = pd.read_csv(test_task_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Mixture 1</th>\n",
       "      <th>Mixture 2</th>\n",
       "      <th>Experimental Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bushdid</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Snitz 2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.640420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Snitz 1</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>0.745192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snitz 2</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>0.608784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Snitz 1</td>\n",
       "      <td>49</td>\n",
       "      <td>45</td>\n",
       "      <td>0.676136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dataset  Mixture 1  Mixture 2  Experimental Values\n",
       "0  Bushdid         65         66             0.653846\n",
       "1  Snitz 2          1          8             0.640420\n",
       "2  Snitz 1         24         17             0.745192\n",
       "3  Snitz 2         12         14             0.608784\n",
       "4  Snitz 1         49         45             0.676136"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Same way preparing features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m_test, y_test_true, num_mixtures, all_pairs_CIDs = format_Xy(test_set,  mixtures_IDs, CID2features_mordred, method = 'avg')\n",
    "X_d_test, _, _, _ = format_Xy(test_set,  mixtures_IDs, CID2features_deepnose, method = 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pairs_m_test = np.array([(np.concatenate((x1, x2))) for x1, x2 in X_m_test])\n",
    "X_pairs_d_test = np.array([(np.concatenate((x1, x2))) for x1, x2 in X_d_test])\n",
    "\n",
    "distances_m_test = [get_euclidean_distance(m[0], m[1]) for m in X_m_test]\n",
    "similarities_m_test = [get_cosine_similarity(m[0], m[1]) for m in X_m_test]\n",
    "angles_m_test = [get_cosine_angle(m[0], m[1]) for m in X_m_test] \n",
    "\n",
    "distances_d_test = [get_euclidean_distance(m[0], m[1]) for m in X_d_test]\n",
    "similarities_d_test = [get_cosine_similarity(m[0], m[1]) for m in X_d_test]\n",
    "angles_d_test = [get_cosine_angle(m[0], m[1]) for m in X_d_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_monos_test = [ len( set(pair[0]).intersection(set(pair[1]))) for pair in all_pairs_CIDs]\n",
    "diff_monos_test = [ len( set(pair[0]).difference(set(pair[1]))) for pair in all_pairs_CIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['Sum num monos'] = np.array(num_mixtures).sum(axis = 1)\n",
    "test_set['Shared'] = shared_monos_test\n",
    "test_set['Diff'] = diff_monos_test\n",
    "test_set['Num mixture1'] = np.array(num_mixtures)[:, 0]\n",
    "test_set['Num mixture2'] = np.array(num_mixtures)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 For the `Dataset` feature that is unavailable, we impute with a KNN inputator\n",
    "\n",
    "A different strategy can just be, to use `Bushdid` as it's the same type of experimental paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1, start with filling NaN first, and then later impute\n",
    "data_arr = np.full((len(test_set), 4), np.nan) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add all information above\n",
    "X_test = np.hstack( (X_pairs_d_test, X_pairs_m_test,\n",
    "                        np.array(distances_m_test).reshape(46, 1), \n",
    "                        np.array(similarities_m_test).reshape(46, 1), \n",
    "                        np.array(angles_m_test).reshape(46, 1), \n",
    "                        np.array(distances_d_test).reshape(46, 1), \n",
    "                        np.array(similarities_d_test).reshape(46, 1), \n",
    "                        np.array(angles_d_test).reshape(46, 1), \n",
    "                        np.array(shared_monos_test).reshape(46, 1), \n",
    "                        np.array(diff_monos_test).reshape(46, 1), \n",
    "                        np.array(num_mixtures).reshape(46,2), \n",
    "                        data_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KNNImputer object\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Fit the imputer on the training data\n",
    "imputer.fit(X_features)\n",
    "\n",
    "# Transform the training data and test data\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "for model in rf_models:\n",
    "    y_pred = model.predict(X_test_imputed)\n",
    "    y_pred_list.append(y_pred)\n",
    "\n",
    "y_pred_avg = np.mean(y_pred_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_set['Experimental Values'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - R: 0.709\n",
      "Random Forest - RMSE: 0.121\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation and R^2 for Random Forest\n",
    "rf_corr = np.corrcoef(y_pred_avg, y_true)[0, 1]\n",
    "rf_rmse = np.sqrt(mean_squared_error(np.array(y_true), y_pred_avg))\n",
    "\n",
    "print(f\"Random Forest - R: {rf_corr:.3f}\")\n",
    "print(f\"Random Forest - RMSE: {rf_rmse:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
